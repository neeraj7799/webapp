<html>
    <head>
        <title>Neeraj's Resume</title>
        <body>
            <h1 style="text-align:center">Neeraj Avutu</h1>
            <p style="text-align:center">India  <a href="https://www.linkedin.com/in/neeraj-reddy-99a4b95b/">https://www.linkedin.com/in/neeraj-reddy-99a4b95b/</a></p>
            <hr>
            <h4><u>Work Experience</u></h4>
            <br>
            <div>
            <h5>Hudson Data</h5>
            <p>Data Engineer</p>
            <p>July 2018-Present</p>
            <ul>
                <li>Adapted to different verticals within the team based on the requirements from client(Fintech)</li>
                <li>Implemented python modules to parse xml or json formatted data, and enriching raw data, creating and limiting
                    variables</li>
                <li>Creating Visualizations for model monitoring, recession monitoring (involves fetching data from difference sources using APIs/scraping) on DOMO</li>
                <li>Designed and implemented framework to fetch data from external APIs – Unemployment rate, delinquency indexes</li>
                <li>Load testing APIs using JMeter and Locust</li>
                <li>Building ETL (Extract, Transform and Loading data) pipelines using Cloud Dataflow (Apache Beam), PySpark for
                    processing XML and JSON Fields.</li>
                <li>Web Crawling 400k websites for a adhoc request using Scrapy module in python.</li>
                <li>Creating Domo Reports or Data Studio reports (BI Tools and Data Visualization) for Ad-hoc requests by pulling data
                    from Hive or MySQL.</li>
                <li>Automation of Data pipelines for data processing, and reporting for model monitoring purposes – Python(pandas, np), SQL, hive</li>
                <li>Using OmniSciDB (formerly MapD) for the implementation of micro-segmentation technique to leverage the full
                    performance of modern hardware (both CPUs and GPUs).</li>
                <li>Maintaining Analytical warehouse by bringing in historical and incremental data to BigQuery from Hive/MySQL.</li>
                <li>Building Models using GBM,RF,XgBoost,GLM(LR), DTs algorithms</li>
                <li>Using/Optimising Forward selection (greedy algorithm) technique for feature selection</li>
                <li>Automation using Selenium module for bringing in data from various websites and storing them in bigquery by scheduling a cron job</li>
                <li>Using Cloud Functions for loading incremental data into BigQuery</li>
                <li>Building a Model for:<ol><li> Predicting probability to default</li>
                <li>Forecasting for the next 15 days who would be affected by COVID </li>

                <li>Likelihood of accepting a loan</li>
            <li></li></ol></li>
            <li>Using Dataproc (Spark) for handling hundreds of TBs of data for extracting data required for modelling purposes.</li>
            <li>Performing Unit Testing for quality checks over the data transferred data from different services.</li>
            <li></li>
                <li>Mentored and trained a individuals in their learning journey in python, mysql, hive, GCP etc. which resulted increased
                    their contribution by 30%.</li>
        </div>
        </body>
    </head>
</html>